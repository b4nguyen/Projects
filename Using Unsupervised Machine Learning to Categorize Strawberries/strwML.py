# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q0lxHWDYeoEM3MYln-NaQcEBi9sJYdAk

# COGS 118B - Final Project

# Explore the World of Apple

## Group members

- Brinda Narayanan
- Leena Jannatipour
- Eric Lin
- Bryan Nguyen
- Yiling Cao

# Abstract
This project is to identify clusters of apples based on their different attributes through unsupervised machine learning. The dataset is sourced from Kaggle, and contains 4000 observations with 8 variables each (size, weight, sweetness, crunchiness, juiciness, ripeness, quality, and acidity). We will be ignoring the true quality label to see if natural quality groupings form. We will be using K-means clustering and GMM to identify the clusters, and we will be comparing the unsupervised results with a baseline of random clustering. We will be using the Silhouette score to measure the similarity of an apple to its cluster. The overall goal of this project is to see how effective unsupervised learning is when applied to agricultural datasets.

# Background

In a highly competitive fruit market, apples have become a common item in a household’s grocery list. Thus, making the quality that is put out important among farmers, distributors, etc. High-quality apples command premium prices in the market. Producers and retailers can achieve higher profit margins by offering top-quality apples that meet consumer expectations and demand<sup><a href="#Leafylives" id="leafylives">[1]</a></sup>.

Prior work done in apple quality is: Research has been done on developing grading and classification systems for apples based on quality attributes, using techniques like machine learning, computer vision, and spectroscopy
These systems can automate the quality assessment process and provide objective and consistent evaluations<sup><a href="#throop" id="throop">[2]</a></sup>. Traditional methods for calculating apple quality typically involve manual inspection, measurement of physical and chemical attributes, and sensory evaluation by trained panels or consumers. While these methods have been used extensively, they have several limitations that have driven the need for machine learning approaches in apple quality assessment.

Using machine learning in apple quality is relatively new, but is something that shows potential, especially in image data. Previous work to cluster apples into their own categories has been done with K-means clustering<sup><a href="#yu" id="yu">[3]</a></sup>. The paper discusses why K-means clustering should be employed. Given that apples possess multiple features that contribute to their overall quality, such features can be represented in a multi-dimensional space, allowing apples with similar attribute values to be grouped together which is why clustering should be done, and thus the most popular one: K-means clustering.
Using machine learning to cluster for apple quality has multiple benefits for the world. Insights from clustering can optimize supply chain processes. For instance, knowing which clusters of apples tend to spoil faster can inform decisions about transportation, storage, and distribution routes, ultimately reducing wastage and ensuring fresher produce reaches consumers. In addition, the automatic grading of apples can help researchers delve deeper into understanding the factors driving differences between apple clusters, leading to innovations in breeding techniques, agricultural practices, and post-harvest technologies.

From Kaggle, we can see other projects done with our dataset. These projects use different types of machine learning models in order to classify and analyze the apples. One of the algorithms did some clustering using k-means with PCA to cluster the data into different points. Another looks at naive bayes in their EDA<sup><a href="#elgiriyewithana" id="elgiriyewithana">[4]</a></sup>.

# Problem Statement

The objective of this research is to apply unsupervised learning techniques to systematically categorize apples into distinct clusters based on measurable physical attributes such as size, color, and weight. This categorization aims to infer quality levels and suitability for different market segments in the absence of explicit quality labels. The importance of solving this problem lies in its potential to revolutionize the agricultural supply chain by providing a quantifiable and scalable method for assessing fruit quality. This methodology addresses a critical gap in current practices where quality assessment is often subjective or based on inconsistent standards.

Hypothesis: It is hypothesized that unsupervised learning can reveal meaningful clusters of apples that correspond with quality levels and predict shelf life, providing a quantifiable, scalable, and replicable method for apple quality assessment across various conditions.


With this problem solved, grocery fruit life can be quantified when apple quality is set in standard. Harvest and storage conditions can also be accurately measured and recorded. The model predictions can be compared against observed outcomes. This model can be replicated to different seasons and regions, making the problem and solution widely applicable.

# Data

- link:https://www.kaggle.com/datasets/nelgiriyewithana/apple-quality/data
- 8 variables and 4000 observations
- An observation consists of size, weight, sweetness, crunchiness, juiciness, ripeness, acidity, and quality.
- We presume the critical variables will be size, weight, sweetness, and acidity, which are all represented by a number scale which appears to be a normal distribution centered at 0.
- It does not appear that there is much need for cleaning in this data; all the data with the exception of “quality” is represented by a normal distribution number scale centered at 0. We may remove the quality column in order to check to see if the model separates different qualities of fruit without the variable.

### Data Cleaning
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install umap-learn
# %pip install --upgrade threadpoolctl

from sklearn.manifold import TSNE
from umap import UMAP
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
file_path = 'apple_quality.csv'
data = pd.read_csv(file_path)
# Realize that the last observation is empty
data = data[:-1]

# Exclude the 'Quality' and 'A_id' columns from the analysis
features = data.drop(['A_id', 'Quality'], axis=1)

# Convert all columns to numeric, ensuring no data type issues
features = features.apply(pd.to_numeric, errors='coerce')

# Filling any possible NaN values with column means
features = features.fillna(features.mean())

"""# Proposed Solution

In order to identify distinct clusters of apples that correspond to their physical attributes (size, weight, sweetness, acidity), we will be employing K-Means Clustering methods for this unsupervised learning problem. In addition to clustering methods, we will also be conducting some dimensionality reduction methods such as PCA, t-SNE, and UMAP, to first visualize our dataset and help us choose the number of clusters that is most suitable for the data.

As a baseline model, we will employ random clustering to measure the effectiveness of our chosen clustering methods.
The method will be K-Means Clustering, this will create clusters based on each data observation’s proximity to the nearest mean. To determine the amount of clusters (k) to use, we will use the Silhouette Score to find the optimal number of clusters for this dataset. This method computes the mean Silhouette Coefficient of all sample, which will show how well the clusters are apart from each other and clearly distinguished.

Pseudocode for K-Means Clustering Algorithm:
Use optimal number of clusters calculated with elbow curve method
Initialize k centroids randomly
Assign each data observation to its nearest centroid
Compute mean of all data points in a cluster, k, and update the centroid values
Repeat until convergence

To test this solution, we will be conducting several experiments with different number of k chosen and different dimensionality reduction method. We will be picking out the highest among all of the models for our final result.

# Evaluation Metrics

One evaluation metric we plan to use to quantify the performance of both our baseline and solution model will be the Silhouette Score. This will allow us to measure the similarity of an apple to its cluster and then compared to other clusters. On a scale from -1 to 1, a high silhouette score indicates a good-fit to its own cluster and poorly matched to other clusters.

# Data Analysis

The apple dataset has many variables, so it is beneficial to perform dimensionality reduction in order to assist clustering methods.

Here, we are using PCA, T-SNE, and UMAP to visualize the data.

### PCA v.s. TSNE v.s. UMAP Visualizations
"""

pca_configs = [2, 3, 5]

# Perform PCA for different n_components and visualize the results for 2 components
for n_components in pca_configs:
    pca = PCA(n_components=n_components)
    pca_result = pca.fit_transform(features)
    explained_variance = np.sum(pca.explained_variance_ratio_) * 100

    print(f"PCA with {n_components} components: Explained variance = {explained_variance:.2f}%")

    if n_components == 2:
        quality_mapping = {'bad': 'red', 'good': 'green'}

        data_clean = data.dropna(subset=['Quality'])

        # Map quality labels to colors again with the cleaned data
        colors_clean = data_clean['Quality'].map(quality_mapping)

        # Perform PCA with 2 components on the cleaned dataset
        features_clean = data_clean.drop(['A_id', 'Quality'], axis=1).apply(pd.to_numeric, errors='coerce').fillna(features.mean())
        pca_2d_quality_clean = PCA(n_components=2)
        pca_2d_quality_result_clean = pca_2d_quality_clean.fit_transform(features_clean)

        # Plot the results with quality labels after cleaning
        plt.figure(figsize=(8, 6))
        plt.scatter(pca_2d_quality_result_clean[:, 0], pca_2d_quality_result_clean[:, 1], c=colors_clean, alpha=0.5)
        plt.title('PCA with 2 components (Colored by Quality)')
        plt.xlabel('Component 1')
        plt.ylabel('Component 2')
        plt.show()

"""From this visualization, we can start to see patterns or clusters forming based on quality, although the separation is not entirely clear-cut. This suggests that while PCA helps in reducing the dimensionality and potentially highlighting some patterns related to fruit quality, the overlap indicates that quality is influenced by a combination of factors that might not be fully captured by the first two principal components alone. Further analysis, potentially using other dimensionality reduction techniques or clustering methods, could provide additional insights.

PCA with 2 components: Explained variance = 46.88%. The visualization above shows the data projected onto two principal components, allowing us to observe the distribution of the data points in this reduced space.

PCA with 3 components: Explained variance = 63.75%. Although we don't visualize 3 components here, increasing the number of components to 3 captures more of the dataset's variance.

PCA with 5 components: Explained variance = 89.05%. Using 5 components captures a significant portion of the dataset's variance, suggesting that these components can represent much of the information in the original dataset.
"""

perplexity_values = [10, 30, 50]

for perplexity in perplexity_values:
    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=1000, random_state=42)
    tsne_result = tsne.fit_transform(features_clean)

    # Plot the results with quality labels
    plt.figure(figsize=(10, 6))
    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=colors_clean, alpha=0.5)
    plt.title(f't-SNE with Perplexity = {perplexity} (Colored by Quality)')
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.show()

"""In the plot with perplexity 30, the data points seem more spread out, and some local groupings are more apparent. In the plot with perplexity 50, the points are more densely packed, and there is less empty space between clusters. This indicates a stronger emphasis on the global structure.

In both plots, there are red and green points intermingled throughout the space, suggesting that there's no clear separation into distinct clusters according to the "Quality" variable. This might mean that the data doesn't have a strong intrinsic structure according to this particular label.
"""

# Perform UMAP
def umap_plot_with_true_labels(features, labels, n_neighbors=15):
    umap_model = UMAP(n_neighbors=n_neighbors, min_dist=0.1, n_components=2, random_state=42)
    umap_results = umap_model.fit_transform(features)

    umap_df = pd.DataFrame(umap_results, columns=['UMAP_1', 'UMAP_2'])
    umap_df['Quality'] = labels

    # Plotting
    plt.figure(figsize=(10, 7))
    unique_labels = umap_df['Quality'].unique()
    for label in unique_labels:
        subset = umap_df[umap_df['Quality'] == label]
        plt.scatter(subset['UMAP_1'], subset['UMAP_2'], alpha=0.5, label=label)


    plt.title(f'UMAP Projection with n_neighbors={n_neighbors}')
    plt.xlabel('UMAP_1')
    plt.ylabel('UMAP_2')
    plt.legend()
    plt.show()

# Testing parameters
umap_plot_with_true_labels(features, data_clean['Quality'], n_neighbors=5)

"""The significant overlap between the blue and orange points indicates that the qualities "good" and "bad" are not distinctly clustered in this representation. This might suggest that the features used for the UMAP projection do not perfectly discriminate between "good" and "bad" apples, or it could reflect inherent complexity or noise in the dataset. From this we can tell something about that this dataset will not be satisfied with only 2 clusters with high dimentional features.

The n_neighbors parameter is set to 5, which focuses on the local structure of the data, potentially at the expense of the global picture.

Despite the overlap, there seems to be some structure in the data, with the formation of two broad areas where each quality type appears more concentrated. Yet, there's no clear boundary between them.

# Results

## Baseline Model

Here, we use K-Means as our Baseline Model.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Train the K-Means model with 2 clusters
kmeans_2_clusters = KMeans(n_clusters=2, random_state=42)
kmeans_2_clusters.fit(features_scaled)
cluster_labels_2 = kmeans_2_clusters.labels_

# Calculate the silhouette score for 2 clusters
silhouette_avg_2 = silhouette_score(features_scaled, cluster_labels_2)

silhouette_avg_2

import matplotlib.pyplot as plt
import seaborn as sns

# We'll try to pick two features for visualization, assuming the dataset has multiple features.
# Selecting the first two features for simplicity.
plt.figure(figsize=(10, 6))
sns.scatterplot(x=features_scaled[:, 0], y=features_scaled[:, 1], hue=cluster_labels_2, palette="viridis", s=50)
plt.title('K-Means Clustering with 2 Clusters')
plt.xlabel('Feature 1 (Scaled)')
plt.ylabel('Feature 2 (Scaled)')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

"""We can see that from both the Sihouette Score and the graph, the K-Means clustering with 2 clusters did not present very well. Datapoints are not well seperated and we see many overlaps on the graph indicating that the two cluster boundaries are less clear base on only 2 clusters.

## Model Selection and Finetuning

Now we want to present different models with K-Means Clustering to find the best choice for k clusters. Once we find the best model, we will also conduct some fintuning to improve the clustering procedure.
"""

from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_samples, silhouette_score

umap_model = UMAP(n_neighbors=5, min_dist=0.1, n_components=2, random_state=42)
umap_results = umap_model.fit_transform(features)

baseline_models = [pca_result, tsne_result, umap_results]
baseline_names = ["PCA", "T-SNE", "UMAP"]
range_n_clusters = [2, 3, 4, 5, 6]



for i in range(len(baseline_models)):
    for n_clusters in range_n_clusters:
        # Initialize the clusterer with n_clusters value and a random generator
        # seed of 10 for reproducibility.
        clusterer = KMeans(n_clusters=n_clusters, random_state=10)
        cluster_labels = clusterer.fit_predict(baseline_models[i])

        # The silhouette_score gives the average value for all the samples.
        # This gives a perspective into the density and separation of the formed
        # clusters
        silhouette_avg = silhouette_score(baseline_models[i], cluster_labels)
        print(
            "For", baseline_names[i], "and n_clusters =",
            n_clusters,
            "The average silhouette_score is :",
            silhouette_avg,
        )

#Silhouette Score has the highest score when the number of neighbors is 4, and the model is UMAP.

"""As you can see from here, UMAP with the n_clusters of 4 has the highest silhouette score, meaning it is the parameters and model that clusters the best among the models.

As such, we will select UMAP as our dimensionality reduction technique and set our number of clusters to 4.

Below we are showing some visualization of the clusterings to better obeserve the patterns.
"""

umap = UMAP(n_neighbors=5, min_dist=0.1, n_components=2, random_state=42)
in_umap = umap.fit_transform(features_clean)

labels_umap = kmeans.fit_predict(in_umap)

plt.figure(figsize=(8, 6))
plt.scatter(in_umap[:, 0], in_umap[:, 1], c=labels_umap, alpha=0.5)

plt.title('K-Means Clustering with UMAP', fontsize=16)
plt.xlabel('UMAP_1')
plt.ylabel('UMAP_2')

n_neighbors_range = [2, 5, 10, 15, 20]
min_dist_range = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

best_score = -1
best_config = None

results = []

for n_neighbors in n_neighbors_range:
    for min_dist in min_dist_range:
        umap_reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, random_state=42)
        umap_embedding = umap_reducer.fit_transform(features_clean)

        kmeans = KMeans(n_clusters=4, random_state=42)
        kmeans.fit(umap_embedding)
        cluster_labels = kmeans.labels_

        silhouette_avg = silhouette_score(umap_embedding, cluster_labels)

        results.append((n_neighbors, min_dist, silhouette_avg))

        # Update the best config
        if silhouette_avg > best_score:
            best_score = silhouette_avg
            best_config = (n_neighbors, min_dist)

for result in results:
    print(f"n_neighbors: {result[0]}, min_dist: {result[1]}, Silhouette Score: {result[2]}")

print("\nBest configuration:")
print(f"n_neighbors: {best_config[0]}, min_dist: {best_config[1]}, Best Silhouette Score: {best_score}")

"""With Silhouette Score = 0.509, we presented our best clustering model for the apple quality dataset. The relatively high silhouette score suggests that the clusters identified by the model are meaningful and well-defined. This supports the model's potential utility in categorizing apples based on their physical attributes, with each cluster likely representing apples that are similar in quality, type, or suitability for specific market segments.

A value of 10 'n_neighbors' suggests that the algorithm looks beyond the immediate locality to capture the structure in a slightly larger neighborhood. This helps in understanding more complex patterns that span groups of data points, making it a good choice for datasets where clusters are not solely defined by very tight local groupings but also by broader relationships.

A min_dist value of 0.0 allows for very tight clusters in the reduced space. It means that even within the low-dimensional space created by UMAP, the algorithm strives to keep the clusters as compact as possible, potentially leading to clearer, more defined cluster boundaries in the subsequent K-Means clustering.

# Discussion
In this report, we've found that the classification of apples is a complex and multi-faceted problem.

### Dimensional Analysis
Dimensionality reduction using UMAP before clustering with K-Means can significantly enhance the interpretability and performance of clustering algorithms on complex datasets. This approach is particularly effective for datasets with many features, where traditional clustering methods might struggle to find meaningful patterns due to the curse of dimensionality.

In the analysis of the dimensionality reduction techniques, we find that the structure of the apples is not very obvious. Moreover, the overlap between the clusters suggests a more complex and non-linear relationship between the features than we initially expected. Some clusters based on certain features could be an indicator, but the lines between good and bad are difficult to discern. UMAP seems to have the most distinction between the two, indicating that a mix of local and global patterns could be at work in determining the quality of an apple.

The 'n_neighbors' parameter controls how UMAP balances local versus global data structure. A smaller value focuses more on preserving the local structure, potentially leading to more fragmented clusters that capture nuanced patterns in the data. A larger value, on the other hand, emphasizes the global structure, potentially leading to broader clusters. The 'min_dist' parameter affects the tightness of the clusters in the reduced space, with smaller values creating tighter clusters.

###Clustering
When it comes to discussing the plots using clustering, k-means with UMAP stood out to us as having the most distinct clusters. This indicates that the features we chose are actually relevant for creating groups of apples. Since UMAP preserves some of both local and global structure, we can infer that the relationship is rather complicated and not straightforward.

### Interpreting the result

In the K-means clustering with UMAP, there are four very clear clusters that are visible. The middle two of these clusters appear to line up almost perfectly with the ground-truth "bad quality" apples that were seen in the original UMAP graph. By examining the centroids of the clusters in the reduced space and mapping them back to the original feature space, we identify the defining characteristics of each cluster. This can inform decisions related to apple sorting, grading, and marketing strategies.

As such, the data may potentially be two different types of apples, each type having a different cluster for a "good" or "bad" apple of that type.

The clusters may also be interpreted as apples with different levels of quality. Apples with high or high-medium quality may be clustered to the left or right, and apples with low or low-medium might have their own clusters in the middle.

As this is an unsupervised project with no ground truth labels, there is no clear correct answer. The approach demonstrates a scalable and replicable method for analyzing fruit quality across different seasons and regions. It provides a quantifiable measure of quality that can be standardized across the industry.

### Limitations

After training both types of models (K-Means and Gaussian), and exploring their performance, one of the largest limiting factors that we had was computing power. With DataHub, the environment had only 4GB of memory. With higher computing power and more time, we could test many more hyperparameters, giving us a much broader range of hyperparameters that could have been explored. This could definitely have played a part in increasing the accuracy of the models.  

While 4000 observations may seem like a reasonable sample size, I'm not sure if its representative of apples in general, and I think more data would change the nature of the problem and define more features in apple.

Collaborating with domain experts, such as horticulturists, food scientists, or sensory analysts, to incorporate their knowledge and insights into the modeling process would make the assessment of apple quality more legitmate.

### Ethics & Privacy

A potential side effect of this project is that it may be biased. For instance, if the apples in the dataset were all a specific type of apple, Red Delicious, for example, its categories may not generalize to other types of apples, which may cause farmers that use this tool to produce suboptimal apples.

Another potential issue is that the project may influence a farmer to optimize a certain trait of apples, which may cause increases in the amount of labor and resources that go into the apple. As such, this may contribute to environmental problems or encourage explotation of workers. In addition, this may encourage overuse of pesticides, which may have negative impacts on the environment and consumers.

# Conclusion
In this project, our goal was to use unsupervised machine learning techniques to see if they could be used to determine the quality of apples. What we found was that using dimensional reduction and clustering methods showed a potential pattern within apple features regarding quality. Although we saw this, it is not a definitive measure due to the variable nature of apple farming and a more comprehensive approach would be needed to be sure. Overall our report shows that there is potential for broader applications of these methods. Beyond the classification of apples, it can also be used to find patterns in a variety of different produce and other items commonly tested for quality.

# Footnotes

<a name="throopnote"></a>1.[^](#throop): Elgiriyewithana N. Apple Quality. *Kaggle*. https://www.kaggle.com/datasets/nelgiriyewithana/apple-quality/code<br>
<a name="yangnote"></a>2.[^](#yang): Yang, Y. et al. (27 November 2019) Automatic grading of apples based on multi-features and weighted K-means clustering algorithm. *Science Direct*. https://www.sciencedirect.com/science/article/pii/S2214317319300794<br>
<a name="leafylivesnote"></a>3.[^](#leafylives): Leafylives. (6 November 2024) The Science Behind Growing High-Quality Apples. *Medium*. https://medium.com/@leafylives.in/the-science-behind-growing-high-quality-apples-163f9dad98e2
"""

